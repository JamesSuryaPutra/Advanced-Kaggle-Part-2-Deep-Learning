<Introduction>
In this exercise, you’ll learn how to improve training outcomes by including an early stopping callback to prevent overfitting. When you're ready, run this next cell to set everything up!

****************
# Setup plotting
import matplotlib.pyplot as plt
plt.style.use('seaborn-whitegrid')
# Set Matplotlib defaults
plt.rc('figure', autolayout=True)
plt.rc('axes', labelweight='bold', labelsize='large',
       titleweight='bold', titlesize=18, titlepad=10)
plt.rc('animation', html='html5')

# Setup feedback system
from learntools.core import binder
binder.bind(globals())
from learntools.deep_learning_intro.ex4 import *
****************

First load the Spotify dataset. Your task will be to predict the popularity of a song based on various audio features, like 'tempo', 'danceability', and 'mode'.

****************
import pandas as pd
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import make_column_transformer
from sklearn.model_selection import GroupShuffleSplit
​
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import callbacks
​
spotify = pd.read_csv('../input/dl-course-data/spotify.csv')
​
X = spotify.copy().dropna()
y = X.pop('track_popularity')
artists = X['track_artist']
​
features_num = ['danceability', 'energy', 'key', 'loudness', 'mode',
                'speechiness', 'acousticness', 'instrumentalness',
                'liveness', 'valence', 'tempo', 'duration_ms']
features_cat = ['playlist_genre']
​
preprocessor = make_column_transformer(
    (StandardScaler(), features_num),
    (OneHotEncoder(), features_cat),
)
​
# We'll do a "grouped" split to keep all of an artist's songs in one
# split or the other. This is to help prevent signal leakage.
def group_split(X, y, group, train_size=0.75):
    splitter = GroupShuffleSplit(train_size=train_size)
    train, test = next(splitter.split(X, y, groups=group))
    return (X.iloc[train], X.iloc[test], y.iloc[train], y.iloc[test])
​
X_train, X_valid, y_train, y_valid = group_split(X, y, artists)
​
X_train = preprocessor.fit_transform(X_train)
X_valid = preprocessor.transform(X_valid)
y_train = y_train / 100 # popularity is on a scale 0-100, so this rescales to 0-1.
y_valid = y_valid / 100
​
input_shape = [X_train.shape[1]]
print("Input shape: {}".format(input_shape))

Input shape: [18]
****************

Let's start with the simplest network, a linear model. This model has low capacity. Run this next cell without any changes to train a linear model on the Spotify dataset.

****************
model = keras.Sequential([
    layers.Dense(1, input_shape=input_shape),
])
model.compile(
    optimizer='adam',
    loss='mae',
)
history = model.fit(
    X_train, y_train,
    validation_data=(X_valid, y_valid),
    batch_size=512,
    epochs=50,
    verbose=0, # suppress output since we'll plot the curves
)
history_df = pd.DataFrame(history.history)
history_df.loc[0:, ['loss', 'val_loss']].plot()
print("Minimum Validation Loss: {:0.4f}".format(history_df['val_loss'].min()));

Minimum Validation Loss: 0.1993
****************

It's not uncommon for the curves to follow a "hockey stick" pattern like you see here. This makes the final part of training hard to see, so let's start at epoch 10 instead.

****************
# Start the plot at epoch 10
history_df.loc[10:, ['loss', 'val_loss']].plot()
print("Minimum Validation Loss: {:0.4f}".format(history_df['val_loss'].min()));

Minimum Validation Loss: 0.1993
****************

<1st step: Evaluate baseline>
What do you think? Would you say this model is underfitting, overfitting, just right?

****************
# View the solution (Run this cell to receive credit!)
q_1.check()

Correct:
The gap between these curves is quite small and the validation loss never increases, so it's more likely that the network is underfitting than overfitting. It would be worth experiment
ing with more capacity to see if that's the case.
****************

Now, let's add some capacity to our network. We'll add three hidden layers with 128 units each. Run the next cell to train the network and see the learning curves.

****************
model = keras.Sequential([
    layers.Dense(128, activation='relu', input_shape=input_shape),
    layers.Dense(64, activation='relu'),
    layers.Dense(1)
])
model.compile(
    optimizer='adam',
    loss='mae',
)
history = model.fit(
    X_train, y_train,
    validation_data=(X_valid, y_valid),
    batch_size=512,
    epochs=50,
)
history_df = pd.DataFrame(history.history)
history_df.loc[:, ['loss', 'val_loss']].plot()
print("Minimum Validation Loss: {:0.4f}".format(history_df['val_loss'].min()));

Epoch 1/50
49/49 [==============================] - 2s 6ms/step - loss: 0.2435 - val_loss: 0.2103
Epoch 2/50
49/49 [==============================] - 0s 4ms/step - loss: 0.2018 - val_loss: 0.2029
Epoch 3/50
49/49 [==============================] - 0s 3ms/step - loss: 0.1964 - val_loss: 0.2023
Epoch 4/50
49/49 [==============================] - 0s 4ms/step - loss: 0.1927 - val_loss: 0.2007
Epoch 5/50
49/49 [==============================] - 0s 3ms/step - loss: 0.1907 - val_loss: 0.1998
Epoch 6/50
49/49 [==============================] - 0s 3ms/step - loss: 0.1892 - val_loss: 0.2002
Epoch 7/50
49/49 [==============================] - 0s 4ms/step - loss: 0.1880 - val_loss: 0.2016
Epoch 8/50
49/49 [==============================] - 0s 4ms/step - loss: 0.1868 - val_loss: 0.2017
Epoch 9/50
49/49 [==============================] - 0s 4ms/step - loss: 0.1854 - val_loss: 0.2007
Epoch 10/50
49/49 [==============================] - 0s 4ms/step - loss: 0.1847 - val_loss: 0.2009
Epoch 11/50
49/49 [==============================] - 0s 4ms/step - loss: 0.1839 - val_loss: 0.2004
Epoch 12/50
49/49 [==============================] - 0s 3ms/step - loss: 0.1829 - val_loss: 0.2000
Epoch 13/50
49/49 [==============================] - 0s 3ms/step - loss: 0.1822 - val_loss: 0.2001
Epoch 14/50
49/49 [==============================] - 0s 4ms/step - loss: 0.1819 - val_loss: 0.2005
Epoch 15/50
49/49 [==============================] - 0s 4ms/step - loss: 0.1814 - val_loss: 0.2000
Epoch 16/50
49/49 [==============================] - 0s 4ms/step - loss: 0.1806 - val_loss: 0.2007
Epoch 17/50
49/49 [==============================] - 0s 4ms/step - loss: 0.1805 - val_loss: 0.2016
Epoch 18/50
49/49 [==============================] - 0s 4ms/step - loss: 0.1792 - val_loss: 0.2015
Epoch 19/50
49/49 [==============================] - 0s 4ms/step - loss: 0.1788 - val_loss: 0.2005
Epoch 20/50
49/49 [==============================] - 0s 3ms/step - loss: 0.1781 - val_loss: 0.2016
Epoch 21/50
49/49 [==============================] - 0s 3ms/step - loss: 0.1780 - val_loss: 0.2023
Epoch 22/50
49/49 [==============================] - 0s 3ms/step - loss: 0.1771 - val_loss: 0.2013
Epoch 23/50
49/49 [==============================] - 0s 3ms/step - loss: 0.1768 - val_loss: 0.2015
Epoch 24/50
49/49 [==============================] - 0s 3ms/step - loss: 0.1758 - val_loss: 0.2036
Epoch 25/50
49/49 [==============================] - 0s 4ms/step - loss: 0.1753 - val_loss: 0.2029
Epoch 26/50
49/49 [==============================] - 0s 4ms/step - loss: 0.1748 - val_loss: 0.2024
Epoch 27/50
49/49 [==============================] - 0s 4ms/step - loss: 0.1740 - val_loss: 0.2026
Epoch 28/50
49/49 [==============================] - 0s 4ms/step - loss: 0.1740 - val_loss: 0.2029
Epoch 29/50
49/49 [==============================] - 0s 4ms/step - loss: 0.1734 - val_loss: 0.2029
Epoch 30/50
49/49 [==============================] - 0s 3ms/step - loss: 0.1738 - val_loss: 0.2048
Epoch 31/50
49/49 [==============================] - 0s 3ms/step - loss: 0.1727 - val_loss: 0.2031
Epoch 32/50
49/49 [==============================] - 0s 3ms/step - loss: 0.1729 - val_loss: 0.2047
Epoch 33/50
49/49 [==============================] - 0s 4ms/step - loss: 0.1719 - val_loss: 0.2026
Epoch 34/50
49/49 [==============================] - 0s 3ms/step - loss: 0.1705 - val_loss: 0.2053
Epoch 35/50
49/49 [==============================] - 0s 4ms/step - loss: 0.1702 - val_loss: 0.2039
Epoch 36/50
49/49 [==============================] - 0s 4ms/step - loss: 0.1698 - val_loss: 0.2047
Epoch 37/50
49/49 [==============================] - 0s 4ms/step - loss: 0.1691 - val_loss: 0.2051
Epoch 38/50
49/49 [==============================] - 0s 4ms/step - loss: 0.1699 - val_loss: 0.2063
Epoch 39/50
49/49 [==============================] - 0s 4ms/step - loss: 0.1684 - val_loss: 0.2042
Epoch 40/50
49/49 [==============================] - 0s 3ms/step - loss: 0.1681 - val_loss: 0.2044
Epoch 41/50
49/49 [==============================] - 0s 3ms/step - loss: 0.1679 - val_loss: 0.2060
Epoch 42/50
49/49 [==============================] - 0s 4ms/step - loss: 0.1672 - val_loss: 0.2065
Epoch 43/50
49/49 [==============================] - 0s 4ms/step - loss: 0.1669 - val_loss: 0.2061
Epoch 44/50
49/49 [==============================] - 0s 4ms/step - loss: 0.1663 - val_loss: 0.2062
Epoch 45/50
49/49 [==============================] - 0s 4ms/step - loss: 0.1662 - val_loss: 0.2072
Epoch 46/50
49/49 [==============================] - 0s 4ms/step - loss: 0.1653 - val_loss: 0.2063
Epoch 47/50
49/49 [==============================] - 0s 4ms/step - loss: 0.1654 - val_loss: 0.2078
Epoch 48/50
49/49 [==============================] - 0s 3ms/step - loss: 0.1648 - val_loss: 0.2064
Epoch 49/50
49/49 [==============================] - 0s 3ms/step - loss: 0.1647 - val_loss: 0.2076
Epoch 50/50
49/49 [==============================] - 0s 3ms/step - loss: 0.1643 - val_loss: 0.2069

Minimum Validation Loss: 0.1998
****************

<2nd step: Add capacity>
What is your evaluation of these curves? Underfitting, overfitting, just right?

****************
# View the solution (Run this cell to receive credit!)
q_2.check()

Correct:
Now the validation loss begins to rise very early, while the training loss continues to decrease. This indicates that the network has begun to overfit. At this point, we would need to
try something to prevent it, either by reducing the number of units or through a method like early stopping.
****************

<3rd step: Define early stopping callback>
Now define an early stopping callback that waits 5 epochs (patience') for a change in validation loss of at least 0.001 (min_delta) and keeps the weights with the best loss (restore_
best_weights).

****************
from tensorflow.keras import callbacks
​
# YOUR CODE HERE: define an early stopping callback
early_stopping = callbacks.EarlyStopping(
    min_delta=0.001,
    patience=5,
    restore_best_weights=True
)
​
# Check your answer
q_3.check()

Correct

# Lines below will give you a hint or solution code
q_3.hint()
q_3.solution()

Hint:
Your solution should look something like:
early_stopping = callbacks.EarlyStopping(
    patience=____,
    min_delta=____,
    restore_best_weights=____,
)

Solution:
early_stopping = callbacks.EarlyStopping(
    patience=5,
    min_delta=0.001,
    restore_best_weights=True,
)
****************

Now run this cell to train the model and get the learning curves. Notice the callbacks argument in model.fit.

****************
model = keras.Sequential([
    layers.Dense(128, activation='relu', input_shape=input_shape),
    layers.Dense(64, activation='relu'),    
    layers.Dense(1)
])
model.compile(
    optimizer='adam',
    loss='mae',
)
history = model.fit(
    X_train, y_train,
    validation_data=(X_valid, y_valid),
    batch_size=512,
    epochs=50,
    callbacks=[early_stopping]
)
history_df = pd.DataFrame(history.history)
history_df.loc[:, ['loss', 'val_loss']].plot()
print("Minimum Validation Loss: {:0.4f}".format(history_df['val_loss'].min()));

Epoch 1/50
49/49 [==============================] - 1s 6ms/step - loss: 0.2155 - val_loss: 0.2050
Epoch 2/50
49/49 [==============================] - 0s 4ms/step - loss: 0.1974 - val_loss: 0.2016
Epoch 3/50
49/49 [==============================] - 0s 4ms/step - loss: 0.1938 - val_loss: 0.2007
Epoch 4/50
49/49 [==============================] - 0s 4ms/step - loss: 0.1909 - val_loss: 0.2001
Epoch 5/50
49/49 [==============================] - 0s 4ms/step - loss: 0.1889 - val_loss: 0.1987
Epoch 6/50
49/49 [==============================] - 0s 4ms/step - loss: 0.1884 - val_loss: 0.1985
Epoch 7/50
49/49 [==============================] - 0s 4ms/step - loss: 0.1862 - val_loss: 0.1979
Epoch 8/50
49/49 [==============================] - 0s 4ms/step - loss: 0.1848 - val_loss: 0.1995
Epoch 9/50
49/49 [==============================] - 0s 4ms/step - loss: 0.1845 - val_loss: 0.2011
Epoch 10/50
49/49 [==============================] - 0s 4ms/step - loss: 0.1844 - val_loss: 0.2004

Minimum Validation Loss: 0.1979
****************

<4th step: Train and interpret>
Was this an improvement compared to training without early stopping?

****************
# View the solution (Run this cell to receive credit!)
q_4.check()

Correct:
The early stopping callback did stop the training once the network began overfitting. Moreover, by including restore_best_weights we still get to keep the model where validation loss
was lowest.
****************
