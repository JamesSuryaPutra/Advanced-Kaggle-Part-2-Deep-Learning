<Introduction>
In the tutorial, we saw how to build deep neural networks by stacking layers inside a Sequential model. By adding an activation function after the hidden layers, we gave the network the abi
lity to learn more complex (non-linear) relationships in the data.

In these exercises, you'll build a neural network with several hidden layers and then explore some activation functions beyond ReLU. Run this next cell to set everything up!

****************
import tensorflow as tf

# Setup plotting
import matplotlib.pyplot as plt

plt.style.use('seaborn-whitegrid')
# Set Matplotlib defaults
plt.rc('figure', autolayout=True)
plt.rc('axes', labelweight='bold', labelsize='large',
       titleweight='bold', titlesize=18, titlepad=10)

# Setup feedback system
from learntools.core import binder
binder.bind(globals())
from learntools.deep_learning_intro.ex2 import *

2024-02-20 13:44:08.873730: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory:
Attempting to register factory for plugin cuDNN when one has already been registered
2024-02-20 13:44:08.873894: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory:
Attempting to register factory for plugin cuFFT when one has already been registered
2024-02-20 13:44:09.081364: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory:
Attempting to register factory for plugin cuBLAS when one has already been registered
/tmp/ipykernel_33/3168024915.py:6: MatplotlibDeprecationWarning:
The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<
style>'. Alternatively, directly use the seaborn API instead.
  plt.style.use('seaborn-whitegrid')
****************

In the Concrete dataset, your task is to predict the compressive strength of concrete manufactured according to various recipes. Run the next code cell without changes to load the data
set.

****************
import pandas as pd

concrete = pd.read_csv('../input/dl-course-data/concrete.csv')
concrete.head()

Cement	BlastFurnaceSlag	FlyAsh	Water	Superplasticizer	CoarseAggregate	FineAggregate	Age	CompressiveStrength
0	540.0	0.0	0.0	162.0	2.5	1040.0	676.0	28	79.99
1	540.0	0.0	0.0	162.0	2.5	1055.0	676.0	28	61.89
2	332.5	142.5	0.0	228.0	0.0	932.0	594.0	270	40.27
3	332.5	142.5	0.0	228.0	0.0	932.0	594.0	365	41.05
4	198.6	132.4	0.0	192.0	0.0	978.4	825.5	360	44.30
****************

<1st step: Input shape>
The target for this task is the column 'CompressiveStrength'. The remaining columns are the features we'll use as inputs. What would be the input shape for this dataset?

****************
# YOUR CODE HERE
input_shape = [8]

# Check your answer
q_1.check()

Correct

# Lines below will give you a hint or solution code
q_1.hint()
q_1.solution()

Hint:
Remember to only count the input features when determining input_shape. You should not count the target (the CompressiveStrength column).

Solution:
input_shape = [8]
# you could also use a 1-tuple, like input_shape = (8,)
****************

<2nd step: Define a model with hidden layers>
Now create a model with three hidden layers, each having 512 units and the ReLU activation. Be sure to include an output layer of one unit and no activation, and also input_shape as an
argument to the first layer.

****************
from tensorflow import keras
from tensorflow.keras import layers
​
# YOUR CODE HERE
model = keras.Sequential([
    layers.Dense(units=512, activation='relu', input_shape=input_shape),
    layers.Dense(units=512, activation='relu'),
    layers.Dense(units=512, activation='relu'),
    layers.Dense(1)
])
​
# Check your answer
q_2.check()

Correct

# Lines below will give you a hint or solution code
q_2.hint()
q_2.solution()

Hint:
Your answer should look something like:
model = keras.Sequential([
    ____
])

Solution:
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    layers.Dense(512, activation='relu', input_shape=input_shape),
    layers.Dense(512, activation='relu'),
    layers.Dense(512, activation='relu'),    
    layers.Dense(1),
])
****************

<3rd step: Activation layers>
Let's explore activations functions some. The usual way of attaching an activation function to a Dense layer is to include it as part of the definition with the activation argument.
Sometimes though you'll want to put some other layer between the Dense layer and its activation function (we'll see an example of this in Lesson 5 with batch normalization). In this
case, we can define the activation in its own Activation layer, like so:
layers.Dense(units=8),
layers.Activation('relu')

This is completely equivalent to the ordinary way: layers.Dense(units=8, activation='relu'). Rewrite the following model so that each activation is in its own Activation layer.

****************
### YOUR CODE HERE: rewrite this to use activation layers
model = keras.Sequential([
    layers.Dense(32, input_shape=[8]),
    layers.Activation('relu'),
    layers.Dense(32),
    layers.Activation('relu'),
    layers.Dense(1)
])

# Check your answer
q_3.check()

Correct

# Lines below will give you a hint or solution code
q_3.hint()
q_3.solution()

Hint:
Your model should look something like:
model = keras.Sequential([
    layers.Dense(____),
    layers.Activation(____),
    layers.Dense(____),
    layers.Activation(____),
    layers.Dense(1),
])

Solution:
model = keras.Sequential([
    layers.Dense(32, input_shape=[8]),
    layers.Activation('relu'),
    layers.Dense(32),
    layers.Activation('relu'),
    layers.Dense(1),
])
****************

<Optional: Alternatives to ReLU>
There is a whole family of variants of the 'relu' activation -- 'elu', 'selu', and 'swish', among others -- all of which you can use in Keras. Sometimes one activation will perform bet
ter than another on a given task, so you could consider experimenting with activations as you develop a model. The ReLU activation tends to do well on most problems, so it's a good one
to start with.

Let's look at the graphs of some of these. Change the activation from 'relu' to one of the others named above. Then run the cell to see the graph (check out the documentation for more
ideas).

****************
# YOUR CODE HERE: Change 'relu' to 'elu', 'selu', 'swish'... or something else
activation_layer = layers.Activation('relu')

x = tf.linspace(-3.0, 3.0, 100)
y = activation_layer(x) # once created, a layer is callable just like a function

plt.figure(dpi=100)
plt.plot(x, y)
plt.xlim(-3, 3)
plt.xlabel("Input")
plt.ylabel("Output")
plt.show()
****************
