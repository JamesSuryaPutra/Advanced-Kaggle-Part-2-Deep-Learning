<Introduction>
In this exercise, you'll build a model to predict hotel cancellations with a binary classifier.

****************
# Setup plotting
import matplotlib.pyplot as plt
plt.style.use('seaborn-whitegrid')
# Set Matplotlib defaults
plt.rc('figure', autolayout=True)
plt.rc('axes', labelweight='bold', labelsize='large',
       titleweight='bold', titlesize=18, titlepad=10)
plt.rc('animation', html='html5')

# Setup feedback system
from learntools.core import binder
binder.bind(globals())
from learntools.deep_learning_intro.ex6 import *
****************

First, load the Hotel Cancellations dataset.

****************
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.pipeline import make_pipeline
from sklearn.compose import make_column_transformer

hotel = pd.read_csv('../input/dl-course-data/hotel.csv')

X = hotel.copy()
y = X.pop('is_canceled')

X['arrival_date_month'] = \
    X['arrival_date_month'].map(
        {'January':1, 'February': 2, 'March':3,
         'April':4, 'May':5, 'June':6, 'July':7,
         'August':8, 'September':9, 'October':10,
         'November':11, 'December':12}
    )

features_num = [
    "lead_time", "arrival_date_week_number",
    "arrival_date_day_of_month", "stays_in_weekend_nights",
    "stays_in_week_nights", "adults", "children", "babies",
    "is_repeated_guest", "previous_cancellations",
    "previous_bookings_not_canceled", "required_car_parking_spaces",
    "total_of_special_requests", "adr",
]
features_cat = [
    "hotel", "arrival_date_month", "meal",
    "market_segment", "distribution_channel",
    "reserved_room_type", "deposit_type", "customer_type",
]

transformer_num = make_pipeline(
    SimpleImputer(strategy="constant"), # there are a few missing values
    StandardScaler(),
)
transformer_cat = make_pipeline(
    SimpleImputer(strategy="constant", fill_value="NA"),
    OneHotEncoder(handle_unknown='ignore'),
)
​
preprocessor = make_column_transformer(
    (transformer_num, features_num),
    (transformer_cat, features_cat),
)

# stratify - make sure classes are evenlly represented across splits
X_train, X_valid, y_train, y_valid = \
    train_test_split(X, y, stratify=y, train_size=0.75)

X_train = preprocessor.fit_transform(X_train)
X_valid = preprocessor.transform(X_valid)

input_shape = [X_train.shape[1]]
****************

<1st step: Define model>
The model we'll use this time will have both batch normalization and dropout layers. To ease reading we've broken the diagram into blocks, but you can define it layer by layer as usual.

****************
from tensorflow import keras
from tensorflow.keras import layers
​
# YOUR CODE HERE: define the model given in the diagram
model = keras.Sequential([
    layers.BatchNormalization(input_shape=input_shape),
    layers.Dense(256, activation='relu'),
    layers.BatchNormalization(),
    layers.Dropout(0.3),
    layers.Dense(256, activation='relu'),
    layers.BatchNormalization(),
    layers.Dropout(0.3),
    layers.Dense(1, activation='sigmoid')
])
​
# Check your answer
q_1.check()

Correct
****************

<2nd step: Add optimizer, loss, and metric>
Now compile the model with the Adam optimizer and binary versions of the cross-entropy loss and accuracy metric.

****************
# YOUR CODE HERE
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['binary_accuracy']
)
​
# Check your answer
q_2.check()

Correct

# Lines below will give you a hint or solution code
q_2.hint()
q_2.solution()

Hint:
Your code should look something like:
model.compile(
optimizer=____,
loss=____,
metrics=____,
)

Solution:
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['binary_accuracy'],
)
****************

Finally, run this cell to train the model and view the learning curves. It may run for around 60 to 70 epochs, which could take a minute or two.

****************
early_stopping = keras.callbacks.EarlyStopping(
    patience=5,
    min_delta=0.001,
    restore_best_weights=True,
)
history = model.fit(
    X_train, y_train,
    validation_data=(X_valid, y_valid),
    batch_size=512,
    epochs=200,
    callbacks=[early_stopping]
)
​
history_df = pd.DataFrame(history.history)
history_df.loc[:, ['loss', 'val_loss']].plot(title="Cross-entropy")
history_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot(title="Accuracy")

Epoch 1/200
175/175 [==============================] - 6s 7ms/step - loss: 0.4834 - binary_accuracy: 0.7708 - val_loss: 0.4323 - val_binary_accuracy: 0.7973
Epoch 2/200
175/175 [==============================] - 1s 5ms/step - loss: 0.4208 - binary_accuracy: 0.8030 - val_loss: 0.4027 - val_binary_accuracy: 0.8135
Epoch 3/200
175/175 [==============================] - 1s 6ms/step - loss: 0.4084 - binary_accuracy: 0.8088 - val_loss: 0.3975 - val_binary_accuracy: 0.8178
Epoch 4/200
175/175 [==============================] - 1s 5ms/step - loss: 0.4021 - binary_accuracy: 0.8123 - val_loss: 0.3912 - val_binary_accuracy: 0.8206
Epoch 5/200
175/175 [==============================] - 1s 5ms/step - loss: 0.3956 - binary_accuracy: 0.8161 - val_loss: 0.3863 - val_binary_accuracy: 0.8213
Epoch 6/200
175/175 [==============================] - 1s 5ms/step - loss: 0.3912 - binary_accuracy: 0.8164 - val_loss: 0.3842 - val_binary_accuracy: 0.8243
Epoch 7/200
175/175 [==============================] - 1s 5ms/step - loss: 0.3885 - binary_accuracy: 0.8197 - val_loss: 0.3817 - val_binary_accuracy: 0.8270
Epoch 8/200
175/175 [==============================] - 1s 5ms/step - loss: 0.3845 - binary_accuracy: 0.8206 - val_loss: 0.3780 - val_binary_accuracy: 0.8300
Epoch 9/200
175/175 [==============================] - 1s 6ms/step - loss: 0.3818 - binary_accuracy: 0.8220 - val_loss: 0.3754 - val_binary_accuracy: 0.8289
Epoch 10/200
175/175 [==============================] - 1s 6ms/step - loss: 0.3798 - binary_accuracy: 0.8247 - val_loss: 0.3740 - val_binary_accuracy: 0.8305
Epoch 11/200
175/175 [==============================] - 1s 5ms/step - loss: 0.3760 - binary_accuracy: 0.8259 - val_loss: 0.3722 - val_binary_accuracy: 0.8322
Epoch 12/200
175/175 [==============================] - 1s 5ms/step - loss: 0.3731 - binary_accuracy: 0.8266 - val_loss: 0.3702 - val_binary_accuracy: 0.8325
Epoch 13/200
175/175 [==============================] - 1s 5ms/step - loss: 0.3720 - binary_accuracy: 0.8286 - val_loss: 0.3678 - val_binary_accuracy: 0.8326
Epoch 14/200
175/175 [==============================] - 1s 5ms/step - loss: 0.3693 - binary_accuracy: 0.8290 - val_loss: 0.3679 - val_binary_accuracy: 0.8330
Epoch 15/200
175/175 [==============================] - 1s 5ms/step - loss: 0.3668 - binary_accuracy: 0.8291 - val_loss: 0.3671 - val_binary_accuracy: 0.8344
Epoch 16/200
175/175 [==============================] - 1s 5ms/step - loss: 0.3653 - binary_accuracy: 0.8310 - val_loss: 0.3667 - val_binary_accuracy: 0.8335
Epoch 17/200
175/175 [==============================] - 1s 5ms/step - loss: 0.3641 - binary_accuracy: 0.8307 - val_loss: 0.3620 - val_binary_accuracy: 0.8362
Epoch 18/200
175/175 [==============================] - 1s 6ms/step - loss: 0.3625 - binary_accuracy: 0.8322 - val_loss: 0.3654 - val_binary_accuracy: 0.8347
Epoch 19/200
175/175 [==============================] - 1s 6ms/step - loss: 0.3605 - binary_accuracy: 0.8332 - val_loss: 0.3613 - val_binary_accuracy: 0.8370
Epoch 20/200
175/175 [==============================] - 1s 5ms/step - loss: 0.3582 - binary_accuracy: 0.8349 - val_loss: 0.3581 - val_binary_accuracy: 0.8379
Epoch 21/200
175/175 [==============================] - 1s 5ms/step - loss: 0.3574 - binary_accuracy: 0.8340 - val_loss: 0.3617 - val_binary_accuracy: 0.8387
Epoch 22/200
175/175 [==============================] - 1s 5ms/step - loss: 0.3550 - binary_accuracy: 0.8373 - val_loss: 0.3573 - val_binary_accuracy: 0.8372
Epoch 23/200
175/175 [==============================] - 1s 5ms/step - loss: 0.3549 - binary_accuracy: 0.8358 - val_loss: 0.3560 - val_binary_accuracy: 0.8402
Epoch 24/200
175/175 [==============================] - 1s 5ms/step - loss: 0.3538 - binary_accuracy: 0.8368 - val_loss: 0.3565 - val_binary_accuracy: 0.8374
Epoch 25/200
175/175 [==============================] - 1s 5ms/step - loss: 0.3532 - binary_accuracy: 0.8374 - val_loss: 0.3553 - val_binary_accuracy: 0.8388
Epoch 26/200
175/175 [==============================] - 1s 5ms/step - loss: 0.3518 - binary_accuracy: 0.8380 - val_loss: 0.3543 - val_binary_accuracy: 0.8392
Epoch 27/200
175/175 [==============================] - 1s 5ms/step - loss: 0.3522 - binary_accuracy: 0.8373 - val_loss: 0.3529 - val_binary_accuracy: 0.8401
Epoch 28/200
175/175 [==============================] - 1s 5ms/step - loss: 0.3482 - binary_accuracy: 0.8391 - val_loss: 0.3541 - val_binary_accuracy: 0.8389
Epoch 29/200
175/175 [==============================] - 1s 5ms/step - loss: 0.3480 - binary_accuracy: 0.8387 - val_loss: 0.3557 - val_binary_accuracy: 0.8386
Epoch 30/200
175/175 [==============================] - 1s 5ms/step - loss: 0.3463 - binary_accuracy: 0.8397 - val_loss: 0.3517 - val_binary_accuracy: 0.8411
Epoch 31/200
175/175 [==============================] - 1s 5ms/step - loss: 0.3463 - binary_accuracy: 0.8396 - val_loss: 0.3543 - val_binary_accuracy: 0.8394
Epoch 32/200
175/175 [==============================] - 1s 5ms/step - loss: 0.3442 - binary_accuracy: 0.8421 - val_loss: 0.3519 - val_binary_accuracy: 0.8416
Epoch 33/200
175/175 [==============================] - 1s 5ms/step - loss: 0.3437 - binary_accuracy: 0.8413 - val_loss: 0.3524 - val_binary_accuracy: 0.8408
Epoch 34/200
175/175 [==============================] - 1s 5ms/step - loss: 0.3434 - binary_accuracy: 0.8417 - val_loss: 0.3504 - val_binary_accuracy: 0.8423
Epoch 35/200
175/175 [==============================] - 1s 5ms/step - loss: 0.3437 - binary_accuracy: 0.8403 - val_loss: 0.3529 - val_binary_accuracy: 0.8390
Epoch 36/200
175/175 [==============================] - 1s 5ms/step - loss: 0.3421 - binary_accuracy: 0.8414 - val_loss: 0.3502 - val_binary_accuracy: 0.8414
Epoch 37/200
175/175 [==============================] - 1s 5ms/step - loss: 0.3405 - binary_accuracy: 0.8428 - val_loss: 0.3507 - val_binary_accuracy: 0.8419
Epoch 38/200
175/175 [==============================] - 1s 5ms/step - loss: 0.3404 - binary_accuracy: 0.8430 - val_loss: 0.3502 - val_binary_accuracy: 0.8426
Epoch 39/200
175/175 [==============================] - 1s 5ms/step - loss: 0.3396 - binary_accuracy: 0.8438 - val_loss: 0.3501 - val_binary_accuracy: 0.8421

<Axes: title={'center': 'Accuracy'}>
****************

<3rd step: Train and evaluate>
What do you think about the learning curves? Does it look like the model underfit or overfit? Was the cross-entropy loss a good stand-in for accuracy?

****************
# View the solution (Run this cell to receive credit!)
q_3.check()

Correct:
Though we can see the training loss continuing to fall, the early stopping callback prevented any overfitting. Moreover, the accuracy rose at the same rate as the cross-entropy fell,
so it appears that minimizing cross-entropy was a good stand-in. All in all, it looks like this training was a success!
****************
